{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBJsOfTypv03"
   },
   "source": [
    "#Code Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5LDID7AH6ol",
    "outputId": "110aacf8-0b2f-4b7d-ec47-4ad8ed15503e"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install transformers\n",
    "!pip install indonlu\n",
    "!pip install nltk\n",
    "!pip install tqdm\n",
    "!git clone https://github.com/indobenchmark/indonlu.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLIUDp5SFbUk"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from indonlu.utils.forward_fn import forward_sequence_classification\n",
    "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
    "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tk7U9WEwiegC",
    "outputId": "308cd1b3-8fd5-420e-83b6-163839e4a992"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwQyIaeEJwp9"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnNStWVoKQei"
   },
   "outputs": [],
   "source": [
    "set_seed(19072021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20E6XQ3h2wjg"
   },
   "source": [
    "**LOAD MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350,
     "referenced_widgets": [
      "c7f88b8e0f8e4a5083a98af50faee042",
      "bbfb3dbc041b4dfa80cded2bb58fbce6",
      "7ba748206ba640fbae32ac7aebbb17f0",
      "a9c620f9c4314f1391d24cc69ad3e451",
      "cd9aeb655d024f55a21615111c5c54d1",
      "5e2a1f4febf94dc39c6bd71fc6c3c03e",
      "93a0e798aa2348f18c56d46cf6eea6b5",
      "a891ce56569e4c5698b5531a04ba0710",
      "7540c1ef502b409aaba10e40f34bf424",
      "7dd15fefef9347878c04289383c9c25c",
      "1b9ca519bd154d1b8effaae11bdc80b7",
      "74b7babd29e740528d10bf23536ad957",
      "9bb6095d560f4dce85debf3cc105f83b",
      "95ea433a7db2406bb08a00e20bc27650",
      "e2b9ee89ff3242c58bc36da74279c1fb",
      "aa38f3414afd4ece85e1d75a3f160621",
      "1bba94f8bb434de7b98efa11bb49262c",
      "9241e47c695a4431b849d15c1fc9dc08",
      "3b915e4797784a1ab6e16e35ded92c1a",
      "9bcc100b578f4d3a96c63d638aa912c4",
      "7f3c09e30259416fbe3df917a0707761",
      "6dd70480a22b4296a344bb3ccf5cdf56",
      "a139ae4013404d2180697ee858af701e",
      "896be77d0101491b92d5c5a376d27482",
      "57da08838a1c4637b9402b8dda6e7d56",
      "cfe0f970398a43108d7a5471a417de6c",
      "e636ed4ff60144a6b7be19a5c862c491",
      "a6af13c9b2df45468b8f33f7ffbeb922",
      "a46ce9f784e14929932da641084cdf04",
      "1162d44838784d6da998e1a31910c05e",
      "de8772edf30e4bf88a74aaa463d4a73e",
      "9ff38a04074d4bc5b2edff4fcf223ee0",
      "d10dd8cebaa04ae5bfaafd8d92281648",
      "6f6a1a7b9c01435ebdbfafbe369977ca",
      "f96302545d634cccb15325d1ba779b44",
      "716f1a6dca3844a39509f3fbe273d958",
      "5fd32720882d487d9deee5972b142e90",
      "77282e9ffda441ae9326b0d8e87eb577",
      "0eba9c23644044a1be74f3437acf1fcf",
      "53d589863a5f40e0b744711f62e35bb0",
      "5483fe679969469b988b07d11a5c7075",
      "b1248124be0e479392e34982efdfff59",
      "2c81411ccbbc4a7b9cc96bbb2391b8c4",
      "8f12931ce9274f3687333fc3f715777a",
      "b47a7fb90f524df6b8977542d58d4d3a",
      "6be85e4d81944841ba16dda3974526eb",
      "0a716d8dd7c946778688e0aa1a138e37",
      "72b878cb1e6947abb033a870b6752df1",
      "c4268ca6cdea4956b45c8ee58e730eb1",
      "08522c33962242eab9ad31869a419442",
      "99a2007ae7cc4c6288472bd04e344ffb",
      "347a0094ef3c44e786a3f64a08fbc4ee",
      "f4394a8d4b8042009cf6e62be6054b0b",
      "3a479f2a51e249c3a9ad84bfbd9aaae5",
      "40169d10e10b4a88a2ea20f446c4364c"
     ]
    },
    "id": "H57hOGjkOnk2",
    "outputId": "a82f7441-be57-483b-ee3f-03277ec8e4f3"
   },
   "outputs": [],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iO-P0h7qelMQ",
    "outputId": "35793ae1-17f8-4eb8-c1e7-777548a097a8"
   },
   "outputs": [],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqoIWs3B3Ko3"
   },
   "source": [
    "**PREPARE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H6QHj4IAOadH",
    "outputId": "c1a563a0-57b6-400d-d5f6-c68e608f3b67"
   },
   "outputs": [],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1jiRHRYd3vJ"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oR7EjqYzNCPB"
   },
   "source": [
    "#Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_1cY9OKn3VG"
   },
   "source": [
    "\n",
    "TOKENIZER NLTK\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwixI68albyU",
    "outputId": "ce0e398a-8ee4-4189-d046-c40e95006d64"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_csv('/content/MRT-TJ-fix.csv')  # Ganti dengan path file CSV Anda\n",
    "\n",
    "# Periksa beberapa contoh data\n",
    "print(df[['full_text', 'sentiment']].head())\n",
    "\n",
    "# Tokenisasi awal menggunakan NLTK\n",
    "df['tokenized_text'] = df['full_text'].apply(lambda x: word_tokenize(str(x).lower()) if pd.notnull(x) else [])\n",
    "\n",
    "# Print beberapa contoh hasil tokenisasi awal\n",
    "print(df[['full_text', 'tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWdMttqntbZV"
   },
   "source": [
    "CONVERSION & REJOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ewqsnv8VuiN3",
    "outputId": "22ecca10-7304-47d2-8e79-2b6032ff0ad2"
   },
   "outputs": [],
   "source": [
    "# Asumsikan Anda sudah memuat data dari slangWord-2023.txt sebagai JSON object\n",
    "with open('/content/slangWordGbgFix.txt', 'r') as file:\n",
    "    data_json = file.read()\n",
    "\n",
    "# Memuat JSON object\n",
    "data_obj = json.loads(data_json)\n",
    "\n",
    "# Fungsi untuk mengubah token slang ke kata baku menggunakan loop\n",
    "def convert_slang_loop(token_list, slang_dict):\n",
    "    normalized_tokens = []\n",
    "    for token in token_list:  # <-- Loop pertama\n",
    "        if token in slang_dict:\n",
    "            normalized_tokens.append(slang_dict[token])\n",
    "        else:\n",
    "            normalized_tokens.append(token)\n",
    "    return normalized_tokens\n",
    "\n",
    "# Terapkan konversi slang ke tokenized_text menggunakan loop\n",
    "normalized_text = []\n",
    "for tokens in tqdm(df['tokenized_text'], desc=\"Converting slang words\"):  # <-- Loop kedua\n",
    "    normalized_text.append(\" \".join(convert_slang_loop(tokens, data_obj)))\n",
    "\n",
    "# Simpan hasil normalisasi ke DataFrame\n",
    "df['normalized_text'] = normalized_text\n",
    "\n",
    "# Print beberapa contoh hasil konversi\n",
    "print(df[['normalized_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bInU_nR0w2IJ",
    "outputId": "56117473-b6fd-4042-8775-36159ad1d7cc"
   },
   "outputs": [],
   "source": [
    "print(df[['tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzoFsujy645b",
    "outputId": "1e0ee5df-f53e-4046-ead2-e488b7a71fdc"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the stopwords from NLTK\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['cleaned_text'] = df['normalized_text'].apply(remove_stopwords)\n",
    "\n",
    "print(df[['normalized_text', 'cleaned_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMLzfFpDTk7k"
   },
   "source": [
    "TOKENIZER BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y55N3IohN-vK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df[['cleaned_text', 'sentiment']], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJOudUwczei9"
   },
   "outputs": [],
   "source": [
    "test_df, val_df = train_test_split(test_df[['cleaned_text', 'sentiment']], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3nEN9fkKzLxR",
    "outputId": "bf0b3f88-26b1-4dbe-c47b-17b872aa5a5c"
   },
   "outputs": [],
   "source": [
    "main_df = df[['cleaned_text', 'sentiment']]\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHdMhd9SSWh1"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.tsv', sep='\\t', header=False, index=False)\n",
    "test_df.to_csv('test_df.tsv', sep='\\t', header=False, index=False)\n",
    "val_df.to_csv('val_df.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_J4LHTmxzwe9",
    "outputId": "8f576378-11bf-45fd-c24a-b3aa3c7b9a92"
   },
   "outputs": [],
   "source": [
    "train_dataset_path = '/content/train_df.tsv'\n",
    "\n",
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9rotiTlSodc",
    "outputId": "49e8b682-aa0e-40fe-ae6e-170245293f26"
   },
   "outputs": [],
   "source": [
    "test_dataset_path = '/content/test_df.tsv'\n",
    "\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7sh9R8gNkXO"
   },
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ue5RPrd524-",
    "outputId": "6a23b901-8193-4742-c485-c0fd7379b72c"
   },
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iv5T6rXX55tj",
    "outputId": "fff7adb8-ca4a-4a1d-a2f0-b58eefcb739b"
   },
   "outputs": [],
   "source": [
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"TEST LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPe1bU2wiDAG",
    "outputId": "b0bfcf13-c185-4ac6-83ba-9899ccb56436"
   },
   "outputs": [],
   "source": [
    "text = 'naik transjakarta selalu kena macet'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jp-pQvwvROq",
    "outputId": "92b4ce5d-070d-4321-9676-1a8f622450f4"
   },
   "outputs": [],
   "source": [
    "val_dataset_path = '/content/val_df.tsv'\n",
    "\n",
    "val_dataset = DocumentSentimentDataset(val_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "val_loader = DocumentSentimentDataLoader(dataset=val_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEFUiBNFkhlc",
    "outputId": "0d85f68e-0971-440d-e92b-c56427efa7f0"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(val_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "results_df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "results_df.to_csv('results.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yB9Zsfg5BjM7"
   },
   "outputs": [],
   "source": [
    "file_path = 'Sentiment_Analysis.pth'\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLdxBtZr3wiN"
   },
   "source": [
    "#Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8fOceyi5sBS",
    "outputId": "e3414189-7cc8-4833-c5b8-d2471fe380cf"
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ICBxPWII30Ps",
    "outputId": "e99896a9-a398-48b8-936e-563004f9471f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a barplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='sentiment')\n",
    "plt.title('Amount of Data for Each Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Function to generate word cloud for each sentiment\n",
    "def generate_wordcloud(text, sentiment):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'WordCloud for {sentiment} Sentiment')\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud for each sentiment\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    text = ' '.join(df[df['sentiment'] == sentiment]['normalized_text'])\n",
    "    generate_wordcloud(text, sentiment)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "F_HeYiM9KvOc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
